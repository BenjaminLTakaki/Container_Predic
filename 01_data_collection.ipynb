{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9810d2ed",
   "metadata": {},
   "source": [
    "# Data Collection for Europe Base Port Container Price Prediction\n",
    "\n",
    "This notebook focuses on gathering all the raw data needed for our 1-week ahead container price forecasting project. We will collect data from three different sources and save it for later processing.\n",
    "\n",
    "## What is data collection?\n",
    "\n",
    "Data collection is the first step in any data science project. It involves gathering raw information from various sources like files, databases, or APIs (Application Programming Interfaces, which are ways for programs to talk to each other over the internet). Think of it like gathering ingredients before cooking a meal.\n",
    "\n",
    "## What are we predicting?\n",
    "\n",
    "**Target**: Europe Base Port container prices (1 week ahead)\n",
    "\n",
    "**Base Ports Definition**: Average shipping cost for a 40-foot container from Shanghai/China to major European ports including Rotterdam (Netherlands), Hamburg (Germany), London (UK), and Antwerp (Belgium).\n",
    "\n",
    "**Why these ports**: These represent the main entry points for Asian goods into Europe and provide a standard benchmark for European route pricing.\n",
    "\n",
    "## Our data sources:\n",
    "\n",
    "1. Shanghai Containerized Freight Index (local CSV file) - Main price data\n",
    "2. Oil prices (from Yahoo Finance API Hopefully) - Cost factor affecting shipping / (Not working looking for work around)\n",
    "3. Geopolitical disruption data (from GDELT via BigQuery) - Black swan event indicators\n",
    "\n",
    "**Note**: We use GDELT data exported from Google BigQuery for historical coverage (2018-2025). This provides weekly disruption metrics including conflict events, severe incidents, and sentiment analysis for shipping-critical regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25b80a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dc6a28",
   "metadata": {},
   "source": [
    "## Step 1: Load the Shanghai Containerized Freight Index\n",
    "\n",
    "This is our main dataset. It contains weekly freight prices for shipping containers from Shanghai to various destinations around the world. We are specifically interested in the Europe Base Port price, which represents the average cost to ship a container from Shanghai/China to major European ports (Rotterdam, Hamburg, London, Antwerp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbad2e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded the CSV file with 385 rows and 5 columns\n",
      "\n",
      "First 5 rows of the raw data:\n",
      "  the period (YYYY-MM-DD)  Comprehensive Index  Europe (Base port)  \\\n",
      "0                1/5/2018               816.58                 888   \n",
      "1               1/12/2018               839.72                 897   \n",
      "2               1/19/2018               840.36                 891   \n",
      "3               1/26/2018               858.60                 907   \n",
      "4                2/2/2018               883.59                 912   \n",
      "\n",
      "   Mediterranean (Base port)  Persian Gulf and Red Sea (Dubai)  \n",
      "0                        738                               433  \n",
      "1                        759                               450  \n",
      "2                        761                               572  \n",
      "3                        772                               631  \n",
      "4                        797                               611  \n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Load the raw CSV file\n",
    "    # header=1 means the actual column names are in the second row (row 1, counting from 0)\n",
    "    raw_df = pd.read_csv('Shanghai_Containerized_Freight_Index.csv', header=1)\n",
    "    print(f\"Successfully loaded the CSV file with {raw_df.shape[0]} rows and {raw_df.shape[1]} columns\")\n",
    "    \n",
    "    # Display the first few rows to see what the data looks like\n",
    "    print(\"\\nFirst 5 rows of the raw data:\")\n",
    "    print(raw_df.head())\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Error: The file 'Shanghai_Containerized_Freight_Index.csv' was not found.\")\n",
    "    print(\"Please make sure it is in the same folder as this notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8a052b",
   "metadata": {},
   "source": [
    "## Step 2: Select and rename the columns we need\n",
    "\n",
    "The dataset has many columns for different routes, but we only need a few for our Europe Base Port prediction. We will select the date column, the overall freight index (SCFI), and the Europe Base Port price.\n",
    "\n",
    "### Why select only these columns?\n",
    "\n",
    "Europe Base Port: Our prediction target - average cost from Shanghai to Rotterdam/Hamburg/London/Antwerp\n",
    "\n",
    "SCFI_Index: Overall freight market indicator (note: highly correlated with Europe prices, so we may exclude it from final models)\n",
    "\n",
    "Date: Needed for time series analysis and chronological ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1e456b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected and renamed columns:\n",
      "['Date', 'SCFI_Index', 'Europe_Base_Price']\n",
      "\n",
      "Dataset now has 385 rows and 3 columns\n",
      "\n",
      "First 5 rows:\n",
      "        Date  SCFI_Index  Europe_Base_Price\n",
      "0   1/5/2018      816.58                888\n",
      "1  1/12/2018      839.72                897\n",
      "2  1/19/2018      840.36                891\n",
      "3  1/26/2018      858.60                907\n",
      "4   2/2/2018      883.59                912\n"
     ]
    }
   ],
   "source": [
    "# Select only the columns we need\n",
    "df_freight = raw_df[['the period (YYYY-MM-DD)', 'Comprehensive Index', 'Europe (Base port)']].copy()\n",
    "\n",
    "# Rename columns to simpler names\n",
    "df_freight.columns = ['Date', 'SCFI_Index', 'Europe_Base_Price']\n",
    "\n",
    "print(\"Selected and renamed columns:\")\n",
    "print(df_freight.columns.tolist())\n",
    "print(f\"\\nDataset now has {df_freight.shape[0]} rows and {df_freight.shape[1]} columns\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df_freight.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defbac9f",
   "metadata": {},
   "source": [
    "## Step 3: Convert date strings to proper date format\n",
    "\n",
    "Right now, the Date column is just text. We need to convert it to a proper datetime format so Python understands it represents actual dates.\n",
    "\n",
    "### What is datetime?\n",
    "\n",
    "datetime is a special data type in Python that represents dates and times. It allows us to do things like sort by date, calculate time differences, and extract parts of a date (like the month or year)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8401c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted 385 out of 385 dates\n",
      "After removing invalid dates: 385 rows remaining\n",
      "\n",
      "Date conversion complete!\n",
      "            SCFI_Index  Europe_Base_Price\n",
      "Date                                     \n",
      "2018-01-05      816.58                888\n",
      "2018-01-12      839.72                897\n",
      "2018-01-19      840.36                891\n",
      "2018-01-26      858.60                907\n",
      "2018-02-02      883.59                912\n"
     ]
    }
   ],
   "source": [
    "df_freight['Date'] = pd.to_datetime(df_freight['Date'], format='%m/%d/%Y', errors='coerce')\n",
    "\n",
    "# Check how many dates were successfully converted\n",
    "valid_dates = df_freight['Date'].notna().sum()\n",
    "total_rows = len(df_freight)\n",
    "print(f\"Successfully converted {valid_dates} out of {total_rows} dates\")\n",
    "\n",
    "# Remove rows where date conversion failed\n",
    "df_freight.dropna(subset=['Date'], inplace=True)\n",
    "print(f\"After removing invalid dates: {len(df_freight)} rows remaining\")\n",
    "\n",
    "# Set the Date column as the index (the row identifier)\n",
    "df_freight.set_index('Date', inplace=True)\n",
    "\n",
    "print(\"\\nDate conversion complete!\")\n",
    "print(df_freight.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede5391d",
   "metadata": {},
   "source": [
    "## Step 4: Convert price columns to numbers\n",
    "\n",
    "Sometimes data is read as text even when it represents numbers. We need to ensure our price columns are in numeric format so we can do calculations with them.\n",
    "\n",
    "### What is numeric conversion?\n",
    "\n",
    "This process takes text that looks like numbers (like \"123.45\") and converts it to actual numbers that Python can use for math operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c89d515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted SCFI_Index to numeric type\n",
      "Converted Europe_Base_Price to numeric type\n",
      "\n",
      "Removed 0 rows with missing values\n",
      "Final freight dataset: 385 rows\n",
      "Date range: 2018-01-05 to 2025-08-22\n",
      "\n",
      "Final cleaned freight data:\n",
      "            SCFI_Index  Europe_Base_Price\n",
      "Date                                     \n",
      "2018-01-05      816.58                888\n",
      "2018-01-12      839.72                897\n",
      "2018-01-19      840.36                891\n",
      "2018-01-26      858.60                907\n",
      "2018-02-02      883.59                912\n"
     ]
    }
   ],
   "source": [
    "# Convert price columns to numeric format\n",
    "for col in ['SCFI_Index', 'Europe_Base_Price']:\n",
    "    df_freight[col] = pd.to_numeric(df_freight[col], errors='coerce')\n",
    "    print(f\"Converted {col} to numeric type\")\n",
    "\n",
    "# Remove any rows with missing values\n",
    "# This ensures we have complete data for all rows\n",
    "before_drop = len(df_freight)\n",
    "df_freight.dropna(inplace=True)\n",
    "after_drop = len(df_freight)\n",
    "\n",
    "print(f\"\\nRemoved {before_drop - after_drop} rows with missing values\")\n",
    "print(f\"Final freight dataset: {after_drop} rows\")\n",
    "print(f\"Date range: {df_freight.index.min().strftime('%Y-%m-%d')} to {df_freight.index.max().strftime('%Y-%m-%d')}\")\n",
    "\n",
    "print(\"\\nFinal cleaned freight data:\")\n",
    "print(df_freight.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d81295",
   "metadata": {},
   "source": [
    "## Step 5: Fetch oil price data\n",
    "\n",
    "Oil prices affect shipping costs since ships use fuel. We will download historical oil prices using multiple fallback methods.\n",
    "\n",
    "### Option 1: Yahoo Finance API (Primary)\n",
    "\n",
    "We try to fetch data from Yahoo Finance using the `yfinance` library. This is free and usually reliable, though it can occasionally fail due to rate limiting or API changes.\n",
    "\n",
    "**Tickers tried:**\n",
    "- **USO**: United States Oil Fund ETF (most reliable)\n",
    "- **CL=F**: WTI Crude Oil Futures\n",
    "- **BZ=F**: Brent Crude Oil Futures  \n",
    "- **XLE**: Energy Select Sector SPDR Fund (energy sector)\n",
    "\n",
    "### Option 2: Manual CSV File (Backup)\n",
    "\n",
    "If Yahoo Finance fails, you can provide your own oil price data:\n",
    "\n",
    "1. **Download oil prices** from one of these sources:\n",
    "   - **EIA (U.S. Energy Information Administration)**: https://www.eia.gov/dnav/pet/hist/RWTCD.htm\n",
    "     - Free, official U.S. government data\n",
    "     - Download as Excel/CSV and save as `oil_prices_manual.csv`\n",
    "   - **FRED (Federal Reserve Economic Data)**: https://fred.stlouisfed.org/series/DCOILWTICO\n",
    "     - Free historical WTI crude oil prices\n",
    "     - Click \"Download\" → CSV format\n",
    "   - **Quandl/Nasdaq Data Link**: https://data.nasdaq.com/\n",
    "     - Free tier available with API key\n",
    "     \n",
    "2. **Format the CSV** with two columns:\n",
    "   ```\n",
    "   Date,Price\n",
    "   2018-01-01,60.37\n",
    "   2018-01-02,61.44\n",
    "   ...\n",
    "   ```\n",
    "   \n",
    "3. **Save as** `oil_prices_manual.csv` in the same directory as this notebook\n",
    "\n",
    "### Option 3: Synthetic Data (Last Resort)\n",
    "\n",
    "If no real data is available, we create synthetic oil prices for testing purposes only. This uses a random walk model with realistic price ranges ($50-$120) but should NOT be used for actual predictions.\n",
    "\n",
    "### Why Brent/WTI Crude Oil?\n",
    "\n",
    "Brent Crude and WTI (West Texas Intermediate) are major oil benchmarks used for global pricing. Their price movements indicate changes in shipping fuel costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35447936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FETCHING OIL/ENERGY PRICE DATA\n",
      "======================================================================\n",
      "\n",
      "Requesting data from 2018-01-05 to 2025-08-22\n",
      "\n",
      "--- Option 1: Trying Yahoo Finance API ---\n",
      "\n",
      "Trying United States Oil Fund ETF (USO)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to get ticker 'USO' reason: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "1 Failed download:\n",
      "['USO']: YFTzMissingError('$%ticker%: possibly delisted; No timezone found')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ No data or insufficient data returned for USO\n",
      "\n",
      "Trying WTI Crude Oil Futures (CL=F)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to get ticker 'CL=F' reason: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "1 Failed download:\n",
      "['CL=F']: YFTzMissingError('$%ticker%: possibly delisted; No timezone found')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ No data or insufficient data returned for CL=F\n",
      "\n",
      "Trying Brent Crude Oil Futures (BZ=F)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to get ticker 'BZ=F' reason: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "1 Failed download:\n",
      "['BZ=F']: YFTzMissingError('$%ticker%: possibly delisted; No timezone found')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ No data or insufficient data returned for BZ=F\n",
      "\n",
      "Trying Energy Select Sector SPDR Fund (XLE)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to get ticker 'XLE' reason: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "1 Failed download:\n",
      "['XLE']: YFTzMissingError('$%ticker%: possibly delisted; No timezone found')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ No data or insufficient data returned for XLE\n",
      "\n",
      "--- Option 2: Trying manual CSV file ---\n",
      "✗ Manual CSV file not found: oil_prices_manual.csv\n",
      "\n",
      "--- Option 3: Creating synthetic placeholder data ---\n",
      "⚠ WARNING: No real oil data available. Creating synthetic data for demonstration.\n",
      "This should only be used for testing. For production, obtain real oil price data.\n",
      "✓ Created 2787 days of synthetic oil price data\n",
      "  Date range: 2018-01-05 to 2025-08-22\n",
      "  Price range: $50.00 to $120.00\n",
      "  Average: $103.58\n",
      "\n",
      "⚠ Remember: This is SYNTHETIC data. Replace with real data for actual predictions.\n",
      "\n",
      "======================================================================\n",
      "OIL DATA COLLECTION COMPLETE\n",
      "======================================================================\n",
      "Total days: 2787\n",
      "Data source: Yahoo Finance\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"FETCHING OIL/ENERGY PRICE DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get the date range from our freight data\n",
    "start_date = df_freight.index.min().strftime('%Y-%m-%d')\n",
    "end_date = df_freight.index.max().strftime('%Y-%m-%d')\n",
    "\n",
    "print(f\"\\nRequesting data from {start_date} to {end_date}\")\n",
    "\n",
    "df_oil = pd.DataFrame()\n",
    "\n",
    "# Option 1: Try Yahoo Finance first (most reliable when it works)\n",
    "print(\"\\n--- Option 1: Trying Yahoo Finance API ---\")\n",
    "\n",
    "oil_tickers = [\n",
    "    ('USO', 'United States Oil Fund ETF'),  # Oil ETF - most reliable\n",
    "    ('CL=F', 'WTI Crude Oil Futures'),  # WTI Futures\n",
    "    ('BZ=F', 'Brent Crude Oil Futures'),  # Brent Futures\n",
    "    ('XLE', 'Energy Select Sector SPDR Fund'),  # Energy sector ETF\n",
    "]\n",
    "\n",
    "for ticker, name in oil_tickers:\n",
    "    print(f\"\\nTrying {name} ({ticker})...\")\n",
    "    \n",
    "    try:\n",
    "        # Download the oil price data with a longer timeout\n",
    "        temp_df = yf.download(ticker, start=start_date, end=end_date, progress=False, timeout=15)\n",
    "\n",
    "        if not temp_df.empty and len(temp_df) > 10:  # Need at least some data\n",
    "            # Sometimes yfinance returns data with multiple column levels, we need to flatten it\n",
    "            if temp_df.columns.nlevels > 1:\n",
    "                temp_df.columns = temp_df.columns.droplevel(1)\n",
    "            \n",
    "            # Keep only the closing price and rename it\n",
    "            df_oil = temp_df[['Close']].rename(columns={'Close': 'Oil_Price'})\n",
    "            \n",
    "            print(f\"✓ Successfully fetched {len(df_oil)} days of oil price data from {name}\")\n",
    "            print(f\"  Date range: {df_oil.index.min().strftime('%Y-%m-%d')} to {df_oil.index.max().strftime('%Y-%m-%d')}\")\n",
    "            print(f\"  Price range: ${df_oil['Oil_Price'].min():.2f} to ${df_oil['Oil_Price'].max():.2f}\")\n",
    "            print(f\"  Average price: ${df_oil['Oil_Price'].mean():.2f}\")\n",
    "            print(\"\\nFirst 5 rows of oil data:\")\n",
    "            print(df_oil.head())\n",
    "            break  # Success! Exit the loop\n",
    "        else:\n",
    "            print(f\"✗ No data or insufficient data returned for {ticker}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error with {ticker}: {str(e)[:150]}\")\n",
    "        continue\n",
    "\n",
    "# Option 2: Try loading from a manual CSV file if API failed\n",
    "if df_oil.empty:\n",
    "    print(\"\\n--- Option 2: Trying manual CSV file ---\")\n",
    "    try:\n",
    "        # Check if user has provided a manual oil price CSV\n",
    "        oil_csv_path = 'oil_prices_manual.csv'\n",
    "        df_oil = pd.read_csv(oil_csv_path, parse_dates=['Date'], index_col='Date')\n",
    "        \n",
    "        # Filter to our date range\n",
    "        df_oil = df_oil[(df_oil.index >= start_date) & (df_oil.index <= end_date)]\n",
    "        \n",
    "        if not df_oil.empty:\n",
    "            # Rename to standard column name\n",
    "            if 'Price' in df_oil.columns:\n",
    "                df_oil = df_oil[['Price']].rename(columns={'Price': 'Oil_Price'})\n",
    "            elif 'Close' in df_oil.columns:\n",
    "                df_oil = df_oil[['Close']].rename(columns={'Close': 'Oil_Price'})\n",
    "            \n",
    "            print(f\"✓ Loaded {len(df_oil)} days from manual CSV: {oil_csv_path}\")\n",
    "            print(f\"  Date range: {df_oil.index.min().strftime('%Y-%m-%d')} to {df_oil.index.max().strftime('%Y-%m-%d')}\")\n",
    "            print(f\"  Price range: ${df_oil['Oil_Price'].min():.2f} to ${df_oil['Oil_Price'].max():.2f}\")\n",
    "        else:\n",
    "            print(f\"✗ CSV file found but no data in date range\")\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"✗ Manual CSV file not found: {oil_csv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error loading manual CSV: {str(e)[:100]}\")\n",
    "\n",
    "# Option 3: Create synthetic oil price data based on historical patterns (last resort)\n",
    "if df_oil.empty:\n",
    "    print(\"\\n--- Option 3: Creating synthetic placeholder data ---\")\n",
    "    print(\"⚠ WARNING: No real oil data available. Creating synthetic data for demonstration.\")\n",
    "    print(\"This should only be used for testing. For production, obtain real oil price data.\")\n",
    "    \n",
    "    # Create date range matching freight data\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    \n",
    "    # Create synthetic oil prices with realistic values and volatility\n",
    "    # Base price around $70-80 with random walk\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    base_price = 75.0\n",
    "    random_walk = np.random.randn(len(date_range)).cumsum() * 2  # Random walk with std=2\n",
    "    synthetic_prices = base_price + random_walk\n",
    "    \n",
    "    # Clip to reasonable range (50-120)\n",
    "    synthetic_prices = np.clip(synthetic_prices, 50, 120)\n",
    "    \n",
    "    df_oil = pd.DataFrame({\n",
    "        'Oil_Price': synthetic_prices\n",
    "    }, index=date_range)\n",
    "    \n",
    "    print(f\"✓ Created {len(df_oil)} days of synthetic oil price data\")\n",
    "    print(f\"  Date range: {df_oil.index.min().strftime('%Y-%m-%d')} to {df_oil.index.max().strftime('%Y-%m-%d')}\")\n",
    "    print(f\"  Price range: ${df_oil['Oil_Price'].min():.2f} to ${df_oil['Oil_Price'].max():.2f}\")\n",
    "    print(f\"  Average: ${df_oil['Oil_Price'].mean():.2f}\")\n",
    "    print(\"\\n⚠ Remember: This is SYNTHETIC data. Replace with real data for actual predictions.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"OIL DATA COLLECTION COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total days: {len(df_oil)}\")\n",
    "print(f\"Data source: {'Yahoo Finance' if not df_oil.empty and 'synthetic' not in str(df_oil.index.name) else 'Synthetic/Manual'}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f759cc",
   "metadata": {},
   "source": [
    "## Step 6: Load geopolitical disruption data from GDELT BigQuery Export\n",
    "\n",
    "Major geopolitical events like port blockages, conflicts in shipping routes, or regional instability can cause sudden price changes (sometimes called \"black swan\" events). We use GDELT (Global Database of Events, Language, and Tone) data to track these disruptions.\n",
    "\n",
    "### Why BigQuery Instead of GDELT Library?\n",
    "\n",
    "**Previous Approach Issues:**\n",
    "- The `gdeltPyR` library downloads entire daily GKG files (millions of rows) before filtering\n",
    "- Rate limiting issues with GDELT's free API when making many requests\n",
    "- Very slow performance due to excessive data transfer\n",
    "\n",
    "**Current Solution: BigQuery Export**\n",
    "\n",
    "We pre-processed GDELT data using Google BigQuery SQL queries to:\n",
    "1. **Server-side filtering**: Only extract events from shipping-critical regions (Egypt, Yemen, China, Singapore, Netherlands, Germany, Iran, Israel, Taiwan, Ukraine, Russia)\n",
    "2. **Aggregate to weekly level**: Match the SCFI freight data frequency (Fridays)\n",
    "3. **Focus on disruptions**: Filter for conflicts (QuadClass 3,4) and severe negative events (GoldsteinScale < -3)\n",
    "4. **Manageable size**: 403 weekly records (2018-2025) instead of millions of daily events\n",
    "\n",
    "### BigQuery Data Structure\n",
    "\n",
    "The exported CSV contains these weekly metrics:\n",
    "- `iso_year`, `week`: ISO 8601 week numbering\n",
    "- `total_events`: All events in shipping-critical regions\n",
    "- `conflict_events`: Military/violent events (QuadClass 3,4)\n",
    "- `severe_events`: Events with strong negative impact (GoldsteinScale < -3)\n",
    "- `avg_impact`: Average GoldsteinScale (impact severity)\n",
    "- `avg_sentiment`: Average media tone\n",
    "- `total_media_mentions`: Volume of news coverage\n",
    "\n",
    "### How This Helps Our Black Swan Detection\n",
    "\n",
    "Our model will learn: *\"When conflict events spike and media mentions surge in shipping-critical regions, prices usually increase within 1-2 weeks\"*\n",
    "\n",
    "We create a composite `disruption_index` that weighs:\n",
    "- Severe events (weight 3.0): Strong price impact\n",
    "- Conflict events (weight 2.0): Moderate price impact  \n",
    "- Media mentions (weight 1.0): Market attention signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b00bcc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOADING GDELT DISRUPTION DATA FROM BIGQUERY EXPORT\n",
      "======================================================================\n",
      "\n",
      "Loading BigQuery export: world_events_dt.csv\n",
      "✓ Loaded 412 weekly records\n",
      "\n",
      "Columns: ['iso_year', 'week', 'total_events', 'conflict_events', 'severe_events', 'avg_impact', 'avg_sentiment', 'total_media_mentions', 'middle_east_events', 'middle_east_impact', 'middle_east_media', 'asia_events', 'asia_impact', 'europe_events', 'europe_impact', 'ukraine_russia_events', 'ukraine_russia_impact', 'military_conflict_events', 'protest_events', 'trade_restriction_events', 'cooperation_breakdown_events', 'extreme_crisis_events', 'worst_event_impact', 'peak_media_attention', 'media_attention_volatility', 'egypt_events', 'egypt_impact', 'yemen_events', 'yemen_impact']\n",
      "\n",
      "First few rows:\n",
      "   iso_year  week  total_events  conflict_events  severe_events  avg_impact  \\\n",
      "0      2018     0         66868            66868          28034   -5.699991   \n",
      "1      2018     1         74501            74501          31761   -5.900558   \n",
      "2      2018     2         70590            70590          27372   -5.675478   \n",
      "3      2018     3         67256            67256          25037   -5.556539   \n",
      "4      2018     4         71529            71529          29242   -5.781128   \n",
      "\n",
      "   avg_sentiment  total_media_mentions  middle_east_events  \\\n",
      "0      -4.553935                356722                   0   \n",
      "1      -4.175928                396438                   0   \n",
      "2      -3.788087                364143                   0   \n",
      "3      -3.631629                354939                   0   \n",
      "4      -3.964416                394482                   0   \n",
      "\n",
      "   middle_east_impact  ...  trade_restriction_events  \\\n",
      "0                 NaN  ...                     13455   \n",
      "1                 NaN  ...                     14800   \n",
      "2                 NaN  ...                     14497   \n",
      "3                 NaN  ...                     13854   \n",
      "4                 NaN  ...                     15013   \n",
      "\n",
      "   cooperation_breakdown_events  extreme_crisis_events  worst_event_impact  \\\n",
      "0                             0                  18628                 -10   \n",
      "1                             0                  23345                 -10   \n",
      "2                             0                  20182                 -10   \n",
      "3                             0                  17648                 -10   \n",
      "4                             0                  21012                 -10   \n",
      "\n",
      "   peak_media_attention  media_attention_volatility  egypt_events  \\\n",
      "0                   295                    5.695122             0   \n",
      "1                   365                    6.106725             0   \n",
      "2                   220                    5.163389             0   \n",
      "3                   400                    6.004732             0   \n",
      "4                   270                    6.307514             0   \n",
      "\n",
      "   egypt_impact  yemen_events  yemen_impact  \n",
      "0           NaN             0           NaN  \n",
      "1           NaN             0           NaN  \n",
      "2           NaN             0           NaN  \n",
      "3           NaN             0           NaN  \n",
      "4           NaN             0           NaN  \n",
      "\n",
      "[5 rows x 29 columns]\n",
      "\n",
      "======================================================================\n",
      "Converting ISO week numbers to Friday dates...\n",
      "======================================================================\n",
      "✓ Converted 412 weeks to Friday dates\n",
      "  Date range: 2017-12-29 to 2026-01-02\n",
      "\n",
      "======================================================================\n",
      "Creating disruption index...\n",
      "======================================================================\n",
      "Disruption index formula:\n",
      "  = (severe_events/1000) * 3.0\n",
      "  + (conflict_events/1000) * 2.0\n",
      "  + (media_mentions/100000) * 1.0\n",
      "\n",
      "Disruption index statistics:\n",
      "  Mean: 237.885\n",
      "  Std:  98.950\n",
      "  Min:  1.703\n",
      "  Max:  687.690\n",
      "\n",
      "======================================================================\n",
      "✅ GDELT DATA LOADED SUCCESSFULLY\n",
      "======================================================================\n",
      "Total weeks: 412\n",
      "Date range: 2017-12-29 to 2026-01-02\n",
      "\n",
      "Features available:\n",
      "  - disruption_index: Composite disruption score\n",
      "  - tone: Average sentiment (-4.08 avg)\n",
      "  - conflict_count: Military/violent events (28908617 total)\n",
      "  - severe_event_count: High-impact events (12912294 total)\n",
      "  - media_mentions: News coverage volume (145450284 total)\n",
      "\n",
      "Sample data:\n",
      "            disruption_index      tone  conflict_count  severe_event_count  \\\n",
      "date                                                                         \n",
      "2017-12-29         221.40522 -4.553935           66868               28034   \n",
      "2018-01-05         248.24938 -4.175928           74501               31761   \n",
      "2018-01-12         226.93743 -3.788087           70590               27372   \n",
      "2018-01-19         213.17239 -3.631629           67256               25037   \n",
      "2018-01-26         234.72882 -3.964416           71529               29242   \n",
      "2018-02-02         270.64893 -4.105912           80198               35339   \n",
      "2018-02-09         273.70637 -4.096256           83549               34115   \n",
      "2018-02-16         262.35162 -4.237703           81413               31772   \n",
      "2018-02-23         229.71541 -3.996159           72369               27019   \n",
      "2018-03-02         220.03908 -3.662924           68680               26390   \n",
      "\n",
      "            media_mentions  \n",
      "date                        \n",
      "2017-12-29          356722  \n",
      "2018-01-05          396438  \n",
      "2018-01-12          364143  \n",
      "2018-01-19          354939  \n",
      "2018-01-26          394482  \n",
      "2018-02-02          423593  \n",
      "2018-02-09          426337  \n",
      "2018-02-16          420962  \n",
      "2018-02-23          392041  \n",
      "2018-03-02          350908  \n",
      "\n",
      "======================================================================\n",
      "TOP 10 DISRUPTION WEEKS (for validation):\n",
      "======================================================================\n",
      "2023-10-13: Disruption=687.69, Conflicts=180010, Severe=106428, Media=838604\n",
      "2022-03-04: Disruption=660.20, Conflicts=192015, Severe=88886, Media=951669\n",
      "2022-02-25: Disruption=631.14, Conflicts=183730, Severe=84838, Media=916111\n",
      "2023-10-20: Disruption=589.21, Conflicts=161233, Severe=86432, Media=744482\n",
      "2024-04-12: Disruption=542.26, Conflicts=145007, Severe=81692, Media=716773\n",
      "2024-09-27: Disruption=538.24, Conflicts=137876, Severe=85232, Media=678786\n",
      "2022-03-11: Disruption=526.92, Conflicts=153096, Severe=71038, Media=761458\n",
      "2022-03-18: Disruption=503.89, Conflicts=142348, Severe=70725, Media=701886\n",
      "2023-10-27: Disruption=502.11, Conflicts=136449, Severe=74259, Media=643314\n",
      "2023-11-03: Disruption=494.06, Conflicts=134360, Severe=73004, Media=632999\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"LOADING GDELT DISRUPTION DATA FROM BIGQUERY EXPORT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load the BigQuery export CSV (updated filename)\n",
    "gdelt_csv_file = 'world_events_dt.csv'\n",
    "\n",
    "try:\n",
    "    print(f\"\\nLoading BigQuery export: {gdelt_csv_file}\")\n",
    "    df_gdelt = pd.read_csv(gdelt_csv_file)\n",
    "    \n",
    "    print(f\"✓ Loaded {len(df_gdelt)} weekly records\")\n",
    "    print(f\"\\nColumns: {df_gdelt.columns.tolist()}\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    print(df_gdelt.head())\n",
    "    \n",
    "    # Convert ISO year + week to actual Friday dates\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"Converting ISO week numbers to Friday dates...\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Handle ISO week edge cases (week 0 and week 53)\n",
    "    def iso_week_to_friday(row):\n",
    "        \"\"\"Convert ISO year + week to the Friday of that week\"\"\"\n",
    "        year = int(row['iso_year'])\n",
    "        week = int(row['week'])\n",
    "        \n",
    "        # Handle week 0 (some years start in previous ISO year)\n",
    "        if week == 0:\n",
    "            # Week 0 is actually the last week of the previous year\n",
    "            # Use the last Friday of the previous year\n",
    "            last_day_prev_year = pd.Timestamp(year=year-1, month=12, day=31)\n",
    "            days_from_friday = (last_day_prev_year.dayofweek - 4) % 7\n",
    "            return last_day_prev_year - pd.Timedelta(days=days_from_friday)\n",
    "        \n",
    "        # Handle week 53 (some years have 53 ISO weeks)\n",
    "        if week == 53:\n",
    "            # Check if this year actually has 53 weeks\n",
    "            last_day = pd.Timestamp(year=year, month=12, day=31)\n",
    "            iso_calendar = last_day.isocalendar()\n",
    "            if iso_calendar[1] == 53:\n",
    "                # Year has 53 weeks, use last Friday\n",
    "                days_from_friday = (last_day.dayofweek - 4) % 7\n",
    "                return last_day - pd.Timedelta(days=days_from_friday)\n",
    "            else:\n",
    "                # This shouldn't happen, but if week 53 doesn't exist, use week 52\n",
    "                return pd.to_datetime(f'{year}-W52-5', format='%Y-W%W-%w')\n",
    "        \n",
    "        # Normal weeks (1-52)\n",
    "        try:\n",
    "            # %Y-W%W-%w format: year-Week-weekday (5=Friday)\n",
    "            date_str = f'{year}-W{week:02d}-5'\n",
    "            return pd.to_datetime(date_str, format='%Y-W%W-%w')\n",
    "        except:\n",
    "            # Fallback for any parsing issues\n",
    "            print(f\"  Warning: Could not parse year={year}, week={week}. Using approximate date.\")\n",
    "            # Approximate: year start + (week * 7 days) to get to Friday\n",
    "            jan_1 = pd.Timestamp(year=year, month=1, day=1)\n",
    "            days_to_add = (week * 7) + (4 - jan_1.dayofweek) % 7\n",
    "            return jan_1 + pd.Timedelta(days=days_to_add)\n",
    "    \n",
    "    # Apply the conversion\n",
    "    df_gdelt['date'] = df_gdelt.apply(iso_week_to_friday, axis=1)\n",
    "    \n",
    "    # Check for any failed conversions\n",
    "    null_dates = df_gdelt['date'].isna().sum()\n",
    "    if null_dates > 0:\n",
    "        print(f\"⚠ Warning: {null_dates} dates could not be converted and will be dropped\")\n",
    "        df_gdelt = df_gdelt.dropna(subset=['date'])\n",
    "    \n",
    "    print(f\"✓ Converted {len(df_gdelt)} weeks to Friday dates\")\n",
    "    print(f\"  Date range: {df_gdelt['date'].min().strftime('%Y-%m-%d')} to {df_gdelt['date'].max().strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    # Create composite disruption index\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"Creating disruption index...\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Normalize and weight different disruption signals\n",
    "    # Severe events have highest weight (3.0), conflicts medium (2.0), media mentions lower (1.0)\n",
    "    df_gdelt['disruption_index'] = (\n",
    "        (df_gdelt['severe_events'] / 1000) * 3.0 +\n",
    "        (df_gdelt['conflict_events'] / 1000) * 2.0 +\n",
    "        (df_gdelt['total_media_mentions'] / 100000) * 1.0\n",
    "    )\n",
    "    \n",
    "    print(\"Disruption index formula:\")\n",
    "    print(\"  = (severe_events/1000) * 3.0\")\n",
    "    print(\"  + (conflict_events/1000) * 2.0\")  \n",
    "    print(\"  + (media_mentions/100000) * 1.0\")\n",
    "    print(f\"\\nDisruption index statistics:\")\n",
    "    print(f\"  Mean: {df_gdelt['disruption_index'].mean():.3f}\")\n",
    "    print(f\"  Std:  {df_gdelt['disruption_index'].std():.3f}\")\n",
    "    print(f\"  Min:  {df_gdelt['disruption_index'].min():.3f}\")\n",
    "    print(f\"  Max:  {df_gdelt['disruption_index'].max():.3f}\")\n",
    "    \n",
    "    # Prepare final dataframe with renamed columns\n",
    "    df_news = df_gdelt[['date', 'disruption_index', 'avg_sentiment', 'conflict_events', \n",
    "                         'severe_events', 'total_media_mentions']].copy()\n",
    "    \n",
    "    # Rename columns to match expected format\n",
    "    df_news.columns = ['date', 'disruption_index', 'tone', 'conflict_count', \n",
    "                       'severe_event_count', 'media_mentions']\n",
    "    \n",
    "    # Set date as index\n",
    "    df_news.set_index('date', inplace=True)\n",
    "    df_news.sort_index(inplace=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"✅ GDELT DATA LOADED SUCCESSFULLY\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Total weeks: {len(df_news)}\")\n",
    "    print(f\"Date range: {df_news.index.min().strftime('%Y-%m-%d')} to {df_news.index.max().strftime('%Y-%m-%d')}\")\n",
    "    print(f\"\\nFeatures available:\")\n",
    "    print(f\"  - disruption_index: Composite disruption score\")\n",
    "    print(f\"  - tone: Average sentiment ({df_news['tone'].mean():.2f} avg)\")\n",
    "    print(f\"  - conflict_count: Military/violent events ({df_news['conflict_count'].sum():.0f} total)\")\n",
    "    print(f\"  - severe_event_count: High-impact events ({df_news['severe_event_count'].sum():.0f} total)\")\n",
    "    print(f\"  - media_mentions: News coverage volume ({df_news['media_mentions'].sum():.0f} total)\")\n",
    "    \n",
    "    print(f\"\\nSample data:\")\n",
    "    print(df_news.head(10))\n",
    "    \n",
    "    # Find major disruption events for validation\n",
    "    print(f\"\\n\" + \"=\" * 70)\n",
    "    print(\"TOP 10 DISRUPTION WEEKS (for validation):\")\n",
    "    print(\"=\" * 70)\n",
    "    top_disruptions = df_news.nlargest(10, 'disruption_index')[['disruption_index', 'conflict_count', 'severe_event_count', 'media_mentions']]\n",
    "    for date, row in top_disruptions.iterrows():\n",
    "        print(f\"{date.strftime('%Y-%m-%d')}: Disruption={row['disruption_index']:.2f}, \"\n",
    "              f\"Conflicts={row['conflict_count']:.0f}, Severe={row['severe_event_count']:.0f}, \"\n",
    "              f\"Media={row['media_mentions']:.0f}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"\\n🚨 ERROR: Could not find {gdelt_csv_file}\")\n",
    "    print(\"Please ensure the BigQuery export CSV is in the same directory as this notebook.\")\n",
    "    print(\"Creating empty DataFrame...\")\n",
    "    df_news = pd.DataFrame()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n🚨 ERROR loading GDELT data: {type(e).__name__}: {e}\")\n",
    "    import traceback\n",
    "    print(traceback.format_exc())\n",
    "    print(\"Creating empty DataFrame...\")\n",
    "    df_news = pd.DataFrame()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0fa59f",
   "metadata": {},
   "source": [
    "## Step 7: Save the collected data\n",
    "\n",
    "We will save all three datasets as CSV files so we can use them in the next notebooks without having to fetch the data again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b011a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved freight data to 'collected_freight_data.csv'\n",
      "  385 weekly records\n",
      "Saved oil price data to 'collected_oil_data.csv'\n",
      "  2787 daily records\n",
      "Saved GDELT disruption data to 'collected_news_data.csv'\n",
      "  412 weekly records\n",
      "  Features: disruption_index, tone, conflict_count, severe_event_count, media_mentions\n",
      "\n",
      "======================================================================\n",
      "DATA COLLECTION COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "Summary of collected data:\n",
      "✓ Freight data: 385 weeks (2018-01-05 to 2025-08-22)\n",
      "✓ Oil data: 2787 days\n",
      "✓ Disruption data: 412 weeks\n",
      "  - Total conflict events: 28908617\n",
      "  - Total severe events: 12912294\n",
      "  - Average disruption index: 237.885\n",
      "\n",
      "Ready for Step 2: Data Understanding\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Save freight data\n",
    "df_freight.to_csv('collected_freight_data.csv')\n",
    "print(\"Saved freight data to 'collected_freight_data.csv'\")\n",
    "print(f\"  {len(df_freight)} weekly records\")\n",
    "\n",
    "# Save oil data if we have it\n",
    "if not df_oil.empty:\n",
    "    df_oil.to_csv('collected_oil_data.csv')\n",
    "    print(\"Saved oil price data to 'collected_oil_data.csv'\")\n",
    "    print(f\"  {len(df_oil)} daily records\")\n",
    "else:\n",
    "    print(\"No oil data to save (Yahoo Finance may be temporarily unavailable)\")\n",
    "\n",
    "# Save news/disruption data if we have it\n",
    "if not df_news.empty:\n",
    "    df_news.to_csv('collected_news_data.csv')\n",
    "    print(\"Saved GDELT disruption data to 'collected_news_data.csv'\")\n",
    "    print(f\"  {len(df_news)} weekly records\")\n",
    "    print(f\"  Features: {', '.join(df_news.columns.tolist())}\")\n",
    "else:\n",
    "    print(\"No GDELT disruption data to save\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DATA COLLECTION COMPLETE!\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nSummary of collected data:\")\n",
    "print(f\"✓ Freight data: {len(df_freight)} weeks ({df_freight.index.min().strftime('%Y-%m-%d')} to {df_freight.index.max().strftime('%Y-%m-%d')})\")\n",
    "print(f\"✓ Oil data: {len(df_oil) if not df_oil.empty else 0} days\")\n",
    "if not df_news.empty:\n",
    "    print(f\"✓ Disruption data: {len(df_news)} weeks\")\n",
    "    print(f\"  - Total conflict events: {df_news['conflict_count'].sum():.0f}\")\n",
    "    print(f\"  - Total severe events: {df_news['severe_event_count'].sum():.0f}\")\n",
    "    print(f\"  - Average disruption index: {df_news['disruption_index'].mean():.3f}\")\n",
    "else:\n",
    "    print(f\"✗ Disruption data: 0 records\")\n",
    "\n",
    "print(\"\\nReady for Step 2: Data Understanding\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-tf-gpu-py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
