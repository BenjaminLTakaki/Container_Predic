{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9810d2ed",
   "metadata": {},
   "source": [
    "# Data Collection for Europe Base Port Container Price Prediction\n",
    "\n",
    "This notebook focuses on gathering all the raw data needed for our 1-week ahead container price forecasting project. We will collect data from three different sources and save it for later processing.\n",
    "\n",
    "## What are we predicting?\n",
    "\n",
    "**Target**: Europe Base Port container prices (1 week ahead)\n",
    "\n",
    "**Base Ports Definition**: Average shipping cost for a 40-foot container from Shanghai/China to major European ports including Rotterdam (Netherlands), Hamburg (Germany), London (UK), and Antwerp (Belgium).\n",
    "\n",
    "**Why these ports**: These represent the main entry points for Asian goods into Europe and provide a standard benchmark for European route pricing.\n",
    "\n",
    "## Our data sources:\n",
    "\n",
    "1. Shanghai Containerized Freight Index (local CSV file) - Main price data\n",
    "2. Crude oil prices (EIA DCOILWTICO dataset) - Cost factor affecting shipping fuel costs\n",
    "3. Geopolitical disruption data (from GDELT via BigQuery) - Black swan event indicators\n",
    "\n",
    "**Note**: We use GDELT data exported from Google BigQuery for historical coverage (2018-2025). This provides weekly disruption metrics including conflict events, severe incidents, and sentiment analysis for shipping-critical regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b80a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dc6a28",
   "metadata": {},
   "source": [
    "## Step 1: Load the Shanghai Containerized Freight Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbad2e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Load the raw CSV file\n",
    "    # header=1 means the actual column names are in the second row (row 1, counting from 0)\n",
    "    raw_df = pd.read_csv('data/Shanghai_Containerized_Freight_Index.csv', header=1)\n",
    "    print(f\"Successfully loaded the CSV file with {raw_df.shape[0]} rows and {raw_df.shape[1]} columns\")\n",
    "    \n",
    "    # Display the first few rows to see what the data looks like\n",
    "    print(\"\\nFirst 5 rows of the raw data:\")\n",
    "    print(raw_df.head())\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Error: The file 'data/Shanghai_Containerized_Freight_Index.csv' was not found.\")\n",
    "    print(\"Please make sure it is in the data folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8a052b",
   "metadata": {},
   "source": [
    "## Step 2: Select and rename the columns we need\n",
    "\n",
    "The dataset has many columns for different routes, but we only need a few for our Europe Base Port prediction. We will select the date column, the overall freight index (SCFI), and the Europe Base Port price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e456b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the columns we need\n",
    "df_freight = raw_df[['the period (YYYY-MM-DD)', 'Comprehensive Index', 'Europe (Base port)']].copy()\n",
    "\n",
    "# Rename columns to simpler names\n",
    "df_freight.columns = ['Date', 'SCFI_Index', 'Europe_Base_Price']\n",
    "\n",
    "print(\"Selected and renamed columns:\")\n",
    "print(df_freight.columns.tolist())\n",
    "print(f\"\\nDataset now has {df_freight.shape[0]} rows and {df_freight.shape[1]} columns\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df_freight.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defbac9f",
   "metadata": {},
   "source": [
    "## Step 3: Convert date strings to proper date format\n",
    "\n",
    "Right now, the Date column is just text. We need to convert it to a proper datetime format so Python understands it represents actual dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8401c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_freight['Date'] = pd.to_datetime(df_freight['Date'], format='%m/%d/%Y', errors='coerce')\n",
    "\n",
    "# Check how many dates were successfully converted\n",
    "valid_dates = df_freight['Date'].notna().sum()\n",
    "total_rows = len(df_freight)\n",
    "print(f\"Successfully converted {valid_dates} out of {total_rows} dates\")\n",
    "\n",
    "# Remove rows where date conversion failed\n",
    "df_freight.dropna(subset=['Date'], inplace=True)\n",
    "print(f\"After removing invalid dates: {len(df_freight)} rows remaining\")\n",
    "\n",
    "# Set the Date column as the index (the row identifier)\n",
    "df_freight.set_index('Date', inplace=True)\n",
    "\n",
    "print(\"\\nDate conversion complete!\")\n",
    "print(df_freight.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede5391d",
   "metadata": {},
   "source": [
    "## Step 4: Convert price columns to numbers\n",
    "\n",
    "Sometimes data is read as text even when it represents numbers. We need to ensure our price columns are in numeric format so we can do calculations with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c89d515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert price columns to numeric format\n",
    "for col in ['SCFI_Index', 'Europe_Base_Price']:\n",
    "    df_freight[col] = pd.to_numeric(df_freight[col], errors='coerce')\n",
    "    print(f\"Converted {col} to numeric type\")\n",
    "\n",
    "# Remove any rows with missing values\n",
    "# This ensures we have complete data for all rows\n",
    "before_drop = len(df_freight)\n",
    "df_freight.dropna(inplace=True)\n",
    "after_drop = len(df_freight)\n",
    "\n",
    "print(f\"\\nRemoved {before_drop - after_drop} rows with missing values\")\n",
    "print(f\"Final freight dataset: {after_drop} rows\")\n",
    "print(f\"Date range: {df_freight.index.min().strftime('%Y-%m-%d')} to {df_freight.index.max().strftime('%Y-%m-%d')}\")\n",
    "\n",
    "print(\"\\nFinal cleaned freight data:\")\n",
    "print(df_freight.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d81295",
   "metadata": {},
   "source": [
    "## Step 5: Load crude oil price data\n",
    "\n",
    "Oil prices affect shipping costs since ships use fuel. We will load historical crude oil prices from the EIA DCOILWTICO dataset.\n",
    "\n",
    "### Data Source: EIA DCOILWTICO\n",
    "\n",
    "We use the **DCOILWTICO** dataset from the U.S. Energy Information Administration (EIA), which provides daily WTI crude oil prices. This is official government data covering 2018 to current date.\n",
    "\n",
    "**File**: `data/DCOILWTICO.csv`\n",
    "**Format**: CSV with columns `observation_date` and `DCOILWTICO`\n",
    "**Frequency**: Daily prices\n",
    "\n",
    "### Why WTI Crude Oil?\n",
    "\n",
    "WTI (West Texas Intermediate) is a major oil benchmark used for global pricing. Its price movements directly indicate changes in shipping fuel costs, making it a critical factor for container shipping economics.\n",
    "\n",
    "### Processing Steps:\n",
    "1. Load the CSV file\n",
    "2. Convert dates to datetime format\n",
    "3. Rename columns for consistency\n",
    "4. Filter to match freight data date range\n",
    "5. Handle missing values (marked as empty strings in EIA data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35447936",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"LOADING CRUDE OIL PRICE DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get the date range from our freight data\n",
    "start_date = df_freight.index.min().strftime('%Y-%m-%d')\n",
    "end_date = df_freight.index.max().strftime('%Y-%m-%d')\n",
    "\n",
    "print(f\"\\nRequesting data from {start_date} to {end_date}\")\n",
    "\n",
    "df_oil = pd.DataFrame()\n",
    "\n",
    "# Load crude oil prices from EIA DCOILWTICO dataset\n",
    "print(\"\\n--- Loading from EIA DCOILWTICO.csv ---\")\n",
    "\n",
    "try:\n",
    "    # Load the EIA crude oil price data\n",
    "    oil_file_path = 'data/DCOILWTICO.csv'\n",
    "    df_oil_raw = pd.read_csv(oil_file_path, parse_dates=['observation_date'], index_col='observation_date')\n",
    "\n",
    "    # Rename the price column for consistency\n",
    "    df_oil_raw = df_oil_raw.rename(columns={'DCOILWTICO': 'Oil_Price'})\n",
    "\n",
    "    # Filter to our date range\n",
    "    df_oil = df_oil_raw[(df_oil_raw.index >= start_date) & (df_oil_raw.index <= end_date)].copy()\n",
    "\n",
    "    # Handle missing values (EIA uses empty strings for missing data)\n",
    "    df_oil['Oil_Price'] = pd.to_numeric(df_oil['Oil_Price'], errors='coerce')\n",
    "\n",
    "    # Remove rows with missing prices\n",
    "    before_clean = len(df_oil)\n",
    "    df_oil.dropna(subset=['Oil_Price'], inplace=True)\n",
    "    after_clean = len(df_oil)\n",
    "\n",
    "    if not df_oil.empty:\n",
    "        print(f\"Success: Loaded {len(df_oil)} days of crude oil price data from {oil_file_path}\")\n",
    "        print(f\"  Date range: {df_oil.index.min().strftime('%Y-%m-%d')} to {df_oil.index.max().strftime('%Y-%m-%d')}\")\n",
    "        print(f\"  Price range: ${df_oil['Oil_Price'].min():.2f} to ${df_oil['Oil_Price'].max():.2f}\")\n",
    "        print(f\"  Average price: ${df_oil['Oil_Price'].mean():.2f}\")\n",
    "        print(f\"  Removed {before_clean - after_clean} rows with missing values\")\n",
    "        print(\"\\nFirst 5 rows of oil data:\")\n",
    "        print(df_oil.head())\n",
    "    else:\n",
    "        print(f\"Error: No valid oil price data found in date range\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{oil_file_path}' was not found.\")\n",
    "    print(\"Please ensure the DCOILWTICO.csv file is in the data folder.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: An error occurred while loading oil data: {str(e)[:150]}\")\n",
    "\n",
    "# Fallback: Create synthetic oil price data if loading failed (last resort)\n",
    "if df_oil.empty:\n",
    "    print(\"\\n--- Fallback: Creating synthetic placeholder data ---\")\n",
    "    print(\"Warning: No real oil data available. Creating synthetic data for demonstration.\")\n",
    "    print(\"This should only be used for testing. For production, obtain the DCOILWTICO.csv file.\")\n",
    "\n",
    "    # Create date range matching freight data\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "\n",
    "    # Create synthetic oil prices with realistic values and volatility\n",
    "    # Base price around $70-80 with random walk\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    base_price = 75.0\n",
    "    random_walk = np.random.randn(len(date_range)).cumsum() * 2  # Random walk with std=2\n",
    "    synthetic_prices = base_price + random_walk\n",
    "\n",
    "    # Clip to reasonable range (50-120)\n",
    "    synthetic_prices = np.clip(synthetic_prices, 50, 120)\n",
    "\n",
    "    df_oil = pd.DataFrame({\n",
    "        'Oil_Price': synthetic_prices\n",
    "    }, index=date_range)\n",
    "\n",
    "    print(f\"Success: Created {len(df_oil)} days of synthetic oil price data\")\n",
    "    print(f\"  Date range: {df_oil.index.min().strftime('%Y-%m-%d')} to {df_oil.index.max().strftime('%Y-%m-%d')}\")\n",
    "    print(f\"  Price range: ${df_oil['Oil_Price'].min():.2f} to ${df_oil['Oil_Price'].max():.2f}\")\n",
    "    print(f\"  Average: ${df_oil['Oil_Price'].mean():.2f}\")\n",
    "    print(\"\\nWarning: This is SYNTHETIC data. Replace with real DCOILWTICO.csv data for actual predictions.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"OIL DATA LOADING COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total days: {len(df_oil)}\")\n",
    "print(f\"Data source: {'EIA DCOILWTICO' if not df_oil.empty and 'synthetic' not in str(df_oil.index.name) else 'Synthetic'}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f759cc",
   "metadata": {},
   "source": [
    "## Step 6: Load the new \"black swan\" geopolitical disruption data\n",
    "\n",
    "We now load the new, richer BigQuery dataset that contains specific geopolitical and black swan event metrics for shipping-critical regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b00bcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"LOADING NEW BLACK SWAN GEOPOLITICAL DISRUPTION DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Helper function to convert ISO year/week to Friday date\n",
    "def iso_to_friday_date(row):\n",
    "    \"\"\"Converts ISO year and week to the Friday of that week.\"\"\"\n",
    "    return datetime.fromisocalendar(int(row['iso_year']), int(row['week']), 5)\n",
    "\n",
    "# Load the new BigQuery export\n",
    "try:\n",
    "    news_file_path = 'data/bq-results-20251021-090045-1761037274833.csv'\n",
    "    df_news_raw = pd.read_csv(news_file_path)\n",
    "    print(f\"Success: Loaded {len(df_news_raw)} weekly records from {news_file_path}\")\n",
    "    print(f\"  Columns: {df_news_raw.columns.tolist()}\")\n",
    "\n",
    "    # Remove week 53 entries as they cause invalid week errors\n",
    "    before_filter = len(df_news_raw)\n",
    "    df_news_raw = df_news_raw[df_news_raw['week'] != 53]\n",
    "    after_filter = len(df_news_raw)\n",
    "    if before_filter != after_filter:\n",
    "        print(f\"Removed {before_filter - after_filter} week 53 entries (invalid weeks)\")\n",
    "\n",
    "    # Convert ISO week to Friday date to match freight data\n",
    "    print(\"\\nConverting ISO week numbers to Friday dates...\")\n",
    "    df_news_raw['date'] = df_news_raw.apply(iso_to_friday_date, axis=1)\n",
    "    df_news_raw['date'] = pd.to_datetime(df_news_raw['date'])\n",
    "    df_news_raw.set_index('date', inplace=True)\n",
    "    print(f\"Success: Converted dates. New range: {df_news_raw.index.min().date()} to {df_news_raw.index.max().date()}\")\n",
    "\n",
    "    # Drop the non-predictive global_worst_event_impact column\n",
    "    if 'global_worst_event_impact' in df_news_raw.columns:\n",
    "        df_news_raw = df_news_raw.drop(columns=['global_worst_event_impact'])\n",
    "        print(\"Success: Dropped 'global_worst_event_impact' column.\")\n",
    "\n",
    "    # Save the cleaned data\n",
    "    df_news_raw.to_csv('collected_news_data.csv')\n",
    "    print(f\"\\nSuccess: New black swan data saved to 'collected_news_data.csv'\")\n",
    "    print(f\"   Contains {len(df_news_raw.columns)} predictive features.\")\n",
    "    print(\"\\nSample of new event data:\")\n",
    "    print(df_news_raw.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{news_file_path}' was not found.\")\n",
    "    print(\"Please ensure the BigQuery export CSV file is in the data folder.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0fa59f",
   "metadata": {},
   "source": [
    "## Step 7: Save the collected data\n",
    "\n",
    "We will save all three datasets as CSV files so we can use them in the next notebooks without having to fetch the data again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b011a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save freight data\n",
    "df_freight.to_csv('collected_freight_data.csv')\n",
    "print(\"Saved freight data to 'collected_freight_data.csv'\")\n",
    "print(f\"  {len(df_freight)} weekly records\")\n",
    "\n",
    "# Save oil data\n",
    "if not df_oil.empty:\n",
    "    df_oil.to_csv('collected_oil_data.csv')\n",
    "    print(\"Saved crude oil price data to 'collected_oil_data.csv'\")\n",
    "    print(f\"  {len(df_oil)} daily records\")\n",
    "else:\n",
    "    print(\"No oil data to save\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DATA COLLECTION COMPLETE!\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nSummary of collected data:\")\n",
    "print(f\"Available: Freight data: {len(df_freight)} weeks ({df_freight.index.min().strftime('%Y-%m-%d')} to {df_freight.index.max().strftime('%Y-%m-%d')})\")\n",
    "print(f\"Available: Oil data: {len(df_oil) if not df_oil.empty else 0} days\")\n",
    "\n",
    "# Black swan disruption data may not exist if the file failed to load; guard against NameError\n",
    "if 'df_news_raw' in globals():\n",
    "    try:\n",
    "        print(f\"Available: Black swan disruption data: {len(df_news_raw)} weeks\")\n",
    "        print(f\"  Features: {', '.join(df_news_raw.columns.tolist())}\")\n",
    "    except Exception:\n",
    "        # If df_news_raw exists but isn't a dataframe or has issues, report its type\n",
    "        print(f\"df_news_raw exists but could not be summarized (type={type(df_news_raw)})\")\n",
    "else:\n",
    "    print(\"Available: Black swan disruption data: 0 weeks (df_news_raw not found)\")\n",
    "\n",
    "print(\"\\nReady for Step 2: Data Understanding and Feature Engineering\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
