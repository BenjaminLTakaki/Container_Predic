{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9810d2ed",
   "metadata": {},
   "source": [
    "# Data Collection for Europe Base Port Container Price Prediction\n",
    "\n",
    "This notebook focuses on gathering all the raw data needed for our 1-week ahead container price forecasting project. We will collect data from three different sources and save it for later processing.\n",
    "\n",
    "## What are we predicting?\n",
    "\n",
    "**Target**: Europe Base Port container prices (1 week ahead)\n",
    "\n",
    "**Base Ports Definition**: Average shipping cost for a 40-foot container from Shanghai/China to major European ports including Rotterdam (Netherlands), Hamburg (Germany), London (UK), and Antwerp (Belgium).\n",
    "\n",
    "**Why these ports**: These represent the main entry points for Asian goods into Europe and provide a standard benchmark for European route pricing.\n",
    "\n",
    "## Our data sources:\n",
    "\n",
    "1. Shanghai Containerized Freight Index (local CSV file) - Main price data\n",
    "2. Crude oil prices (EIA DCOILWTICO dataset) - Cost factor affecting shipping fuel costs\n",
    "3. Geopolitical disruption data (from GDELT via BigQuery) - Black swan event indicators\n",
    "\n",
    "**Note**: We use GDELT data exported from Google BigQuery for historical coverage (2018-2025). This provides weekly disruption metrics including conflict events, severe incidents, and sentiment analysis for shipping-critical regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25b80a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import plotly.express as px\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dc6a28",
   "metadata": {},
   "source": [
    "## Step 1: Load the Shanghai Containerized Freight Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbad2e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded the CSV file with 385 rows and 5 columns\n",
      "\n",
      "First 5 rows of the raw data:\n",
      "  the period (YYYY-MM-DD)  Comprehensive Index  Europe (Base port)  \\\n",
      "0                1/5/2018               816.58                 888   \n",
      "1               1/12/2018               839.72                 897   \n",
      "2               1/19/2018               840.36                 891   \n",
      "3               1/26/2018               858.60                 907   \n",
      "4                2/2/2018               883.59                 912   \n",
      "\n",
      "   Mediterranean (Base port)  Persian Gulf and Red Sea (Dubai)  \n",
      "0                        738                               433  \n",
      "1                        759                               450  \n",
      "2                        761                               572  \n",
      "3                        772                               631  \n",
      "4                        797                               611  \n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Load the raw CSV file\n",
    "    # header=1 means the actual column names are in the second row (row 1, counting from 0)\n",
    "    raw_df = pd.read_csv('data/raw/Shanghai_Containerized_Freight_Index.csv', header=1)\n",
    "    print(f\"Successfully loaded the CSV file with {raw_df.shape[0]} rows and {raw_df.shape[1]} columns\")\n",
    "    \n",
    "    # Display the first few rows to see what the data looks like\n",
    "    print(\"\\nFirst 5 rows of the raw data:\")\n",
    "    print(raw_df.head())\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Error: The file 'data/raw/Shanghai_Containerized_Freight_Index.csv' was not found.\")\n",
    "    print(\"Please make sure it is in the data/raw folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8a052b",
   "metadata": {},
   "source": [
    "## Step 2: Select and rename the columns we need\n",
    "\n",
    "The dataset has many columns for different routes, but we only need a few for our Europe Base Port prediction. We will select the date column, the overall freight index (SCFI), and the Europe Base Port price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1e456b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected and renamed columns:\n",
      "['Date', 'SCFI_Index', 'Europe_Base_Price']\n",
      "\n",
      "Dataset now has 385 rows and 3 columns\n",
      "\n",
      "First 5 rows:\n",
      "        Date  SCFI_Index  Europe_Base_Price\n",
      "0   1/5/2018      816.58                888\n",
      "1  1/12/2018      839.72                897\n",
      "2  1/19/2018      840.36                891\n",
      "3  1/26/2018      858.60                907\n",
      "4   2/2/2018      883.59                912\n"
     ]
    }
   ],
   "source": [
    "# Select only the columns we need\n",
    "df_freight = raw_df[['the period (YYYY-MM-DD)', 'Comprehensive Index', 'Europe (Base port)']].copy()\n",
    "\n",
    "# Rename columns to simpler names\n",
    "df_freight.columns = ['Date', 'SCFI_Index', 'Europe_Base_Price']\n",
    "\n",
    "print(\"Selected and renamed columns:\")\n",
    "print(df_freight.columns.tolist())\n",
    "print(f\"\\nDataset now has {df_freight.shape[0]} rows and {df_freight.shape[1]} columns\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df_freight.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defbac9f",
   "metadata": {},
   "source": [
    "## Step 3: Convert date strings to proper date format\n",
    "\n",
    "Right now, the Date column is just text. We need to convert it to a proper datetime format so Python understands it represents actual dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8401c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted 385 out of 385 dates\n",
      "After removing invalid dates: 385 rows remaining\n",
      "\n",
      "Date conversion complete!\n",
      "            SCFI_Index  Europe_Base_Price\n",
      "Date                                     \n",
      "2018-01-05      816.58                888\n",
      "2018-01-12      839.72                897\n",
      "2018-01-19      840.36                891\n",
      "2018-01-26      858.60                907\n",
      "2018-02-02      883.59                912\n"
     ]
    }
   ],
   "source": [
    "df_freight['Date'] = pd.to_datetime(df_freight['Date'], format='%m/%d/%Y', errors='coerce')\n",
    "\n",
    "# Check how many dates were successfully converted\n",
    "valid_dates = df_freight['Date'].notna().sum()\n",
    "total_rows = len(df_freight)\n",
    "print(f\"Successfully converted {valid_dates} out of {total_rows} dates\")\n",
    "\n",
    "# Remove rows where date conversion failed\n",
    "df_freight.dropna(subset=['Date'], inplace=True)\n",
    "print(f\"After removing invalid dates: {len(df_freight)} rows remaining\")\n",
    "\n",
    "# Set the Date column as the index (the row identifier)\n",
    "df_freight.set_index('Date', inplace=True)\n",
    "\n",
    "print(\"\\nDate conversion complete!\")\n",
    "print(df_freight.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede5391d",
   "metadata": {},
   "source": [
    "## Step 4: Convert price columns to numbers\n",
    "\n",
    "Sometimes data is read as text even when it represents numbers. We need to ensure our price columns are in numeric format so we can do calculations with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c89d515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted SCFI_Index to numeric type\n",
      "Converted Europe_Base_Price to numeric type\n",
      "\n",
      "Removed 0 rows with missing values\n",
      "Final freight dataset: 385 rows\n",
      "Date range: 2018-01-05 to 2025-08-22\n",
      "\n",
      "Final cleaned freight data:\n",
      "            SCFI_Index  Europe_Base_Price\n",
      "Date                                     \n",
      "2018-01-05      816.58                888\n",
      "2018-01-12      839.72                897\n",
      "2018-01-19      840.36                891\n",
      "2018-01-26      858.60                907\n",
      "2018-02-02      883.59                912\n"
     ]
    }
   ],
   "source": [
    "# Convert price columns to numeric format\n",
    "for col in ['SCFI_Index', 'Europe_Base_Price']:\n",
    "    df_freight[col] = pd.to_numeric(df_freight[col], errors='coerce')\n",
    "    print(f\"Converted {col} to numeric type\")\n",
    "\n",
    "# Remove any rows with missing values\n",
    "# This ensures we have complete data for all rows\n",
    "before_drop = len(df_freight)\n",
    "df_freight.dropna(inplace=True)\n",
    "after_drop = len(df_freight)\n",
    "\n",
    "print(f\"\\nRemoved {before_drop - after_drop} rows with missing values\")\n",
    "print(f\"Final freight dataset: {after_drop} rows\")\n",
    "print(f\"Date range: {df_freight.index.min().strftime('%Y-%m-%d')} to {df_freight.index.max().strftime('%Y-%m-%d')}\")\n",
    "\n",
    "print(\"\\nFinal cleaned freight data:\")\n",
    "print(df_freight.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d81295",
   "metadata": {},
   "source": [
    "## Step 5: Load crude oil price data\n",
    "\n",
    "Oil prices affect shipping costs since ships use fuel. We will load historical crude oil prices from both EIA WTI and Brent EU datasets.\n",
    "\n",
    "### Data Sources: DCOILBRENTEU\n",
    "\n",
    "- **DCOILBRENTEU**: Brent EU crude oil prices\n",
    "\n",
    "**Files**: `data/DCOILBRENTEU.csv`\n",
    "**Format**: CSV with columns `observation_date` and price column\n",
    "**Frequency**: Daily prices\n",
    "\n",
    "### Processing Steps:\n",
    "1. Load both CSV files\n",
    "2. Convert dates to datetime format\n",
    "3. Rename columns for consistency\n",
    "4. Filter to match freight data date range\n",
    "5. Handle missing values (marked as empty strings in EIA data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35447936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOADING CRUDE OIL PRICE DATA\n",
      "======================================================================\n",
      "\n",
      "Requesting data from 2018-01-05 to 2025-08-22\n",
      "Success: Loaded 399 days of Brent EU crude oil price data from data/raw/DCOILBRENTEU.csv\n",
      "  Date range: 2018-01-05 to 2025-08-22\n",
      "  Price range: $15.87 to $127.44\n",
      "  Average price: $73.04\n",
      "  Removed 0 rows with missing values\n",
      "\n",
      "First 5 rows of Brent EU data:\n",
      "                  Brent_Price\n",
      "observation_date             \n",
      "2018-01-05              68.01\n",
      "2018-01-12              69.64\n",
      "2018-01-19              68.56\n",
      "2018-01-26              70.08\n",
      "2018-02-02              67.45\n",
      "\n",
      "======================================================================\n",
      "OIL DATA LOADING COMPLETE\n",
      "======================================================================\n",
      "Total Brent EU days: 399\n",
      "Data sources: EIA DCOILBRENTEU\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"LOADING CRUDE OIL PRICE DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get the date range from our freight data\n",
    "start_date = df_freight.index.min().strftime('%Y-%m-%d')\n",
    "end_date = df_freight.index.max().strftime('%Y-%m-%d')\n",
    "\n",
    "print(f\"\\nRequesting data from {start_date} to {end_date}\")\n",
    "\n",
    "df_oil_wti = pd.DataFrame()\n",
    "df_oil_brent = pd.DataFrame()\n",
    "\n",
    "# Load WTI crude oil prices from EIA DCOILWTICO dataset\n",
    "print(\"\\n--- Loading WTI from EIA DCOILWTICO.csv ---\")\n",
    "\n",
    "try:\n",
    "    # Load the EIA WTI crude oil price data\n",
    "    wti_file_path = 'data/raw/DCOILWTICO.csv'\n",
    "    df_oil_wti_raw = pd.read_csv(wti_file_path, parse_dates=['observation_date'], index_col='observation_date')\n",
    "\n",
    "    # Rename the price column for consistency\n",
    "    df_oil_wti_raw = df_oil_wti_raw.rename(columns={'DCOILWTICO': 'WTI_Price'})\n",
    "\n",
    "    # Filter to our date range\n",
    "    df_oil_wti = df_oil_wti_raw[(df_oil_wti_raw.index >= start_date) & (df_oil_wti_raw.index <= end_date)].copy()\n",
    "\n",
    "    # Handle missing values (EIA uses empty strings for missing data)\n",
    "    df_oil_wti['WTI_Price'] = pd.to_numeric(df_oil_wti['WTI_Price'], errors='coerce')\n",
    "\n",
    "    # Remove rows with missing prices\n",
    "    before_clean = len(df_oil_wti)\n",
    "    df_oil_wti.dropna(subset=['WTI_Price'], inplace=True)\n",
    "    after_clean = len(df_oil_wti)\n",
    "\n",
    "    if not df_oil_wti.empty:\n",
    "        print(f\"Success: Loaded {len(df_oil_wti)} days of WTI crude oil price data from {wti_file_path}\")\n",
    "        print(f\"  Date range: {df_oil_wti.index.min().strftime('%Y-%m-%d')} to {df_oil_wti.index.max().strftime('%Y-%m-%d')}\")\n",
    "        print(f\"  Price range: ${df_oil_wti['WTI_Price'].min():.2f} to ${df_oil_wti['WTI_Price'].max():.2f}\")\n",
    "        print(f\"  Average price: ${df_oil_wti['WTI_Price'].mean():.2f}\")\n",
    "        print(f\"  Removed {before_clean - after_clean} rows with missing values\")\n",
    "        print(\"\\nFirst 5 rows of WTI data:\")\n",
    "        print(df_oil_wti.head())\n",
    "    else:\n",
    "        print(f\"Error: No valid WTI oil price data found in date range\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{wti_file_path}' was not found.\")\n",
    "    print(\"Please ensure the DCOILWTICO.csv file is in the data/raw folder.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: An error occurred while loading WTI data: {str(e)[:150]}\")\n",
    "\n",
    "# Load Brent EU crude oil prices from EIA DCOILBRENTEU dataset\n",
    "print(\"\\n--- Loading Brent EU from EIA DCOILBRENTEU.csv ---\")\n",
    "\n",
    "try:\n",
    "    # Load the EIA Brent EU crude oil price data\n",
    "    brent_file_path = 'data/raw/DCOILBRENTEU.csv'\n",
    "    df_oil_brent_raw = pd.read_csv(brent_file_path, parse_dates=['observation_date'], index_col='observation_date')\n",
    "\n",
    "    # Rename the price column for consistency\n",
    "    df_oil_brent_raw = df_oil_brent_raw.rename(columns={'DCOILBRENTEU': 'Brent_Price'})\n",
    "\n",
    "    # Filter to our date range\n",
    "    df_oil_brent = df_oil_brent_raw[(df_oil_brent_raw.index >= start_date) & (df_oil_brent_raw.index <= end_date)].copy()\n",
    "\n",
    "    # Handle missing values (EIA uses empty strings for missing data)\n",
    "    df_oil_brent['Brent_Price'] = pd.to_numeric(df_oil_brent['Brent_Price'], errors='coerce')\n",
    "\n",
    "    # Remove rows with missing prices\n",
    "    before_clean = len(df_oil_brent)\n",
    "    df_oil_brent.dropna(subset=['Brent_Price'], inplace=True)\n",
    "    after_clean = len(df_oil_brent)\n",
    "\n",
    "    if not df_oil_brent.empty:\n",
    "        print(f\"Success: Loaded {len(df_oil_brent)} days of Brent EU crude oil price data from {brent_file_path}\")\n",
    "        print(f\"  Date range: {df_oil_brent.index.min().strftime('%Y-%m-%d')} to {df_oil_brent.index.max().strftime('%Y-%m-%d')}\")\n",
    "        print(f\"  Price range: ${df_oil_brent['Brent_Price'].min():.2f} to ${df_oil_brent['Brent_Price'].max():.2f}\")\n",
    "        print(f\"  Average price: ${df_oil_brent['Brent_Price'].mean():.2f}\")\n",
    "        print(f\"  Removed {before_clean - after_clean} rows with missing values\")\n",
    "        print(\"\\nFirst 5 rows of Brent EU data:\")\n",
    "        print(df_oil_brent.head())\n",
    "    else:\n",
    "        print(f\"Error: No valid Brent EU oil price data found in date range\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{brent_file_path}' was not found.\")\n",
    "    print(\"Please ensure the DCOILBRENTEU.csv file is in the data/raw folder.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: An error occurred while loading Brent EU data: {str(e)[:150]}\")\n",
    "\n",
    "# Fallback: Create synthetic oil price data if loading failed (last resort)\n",
    "if df_oil_wti.empty and df_oil_brent.empty:\n",
    "    print(\"\\n--- Fallback: Creating synthetic placeholder data ---\")\n",
    "    print(\"Warning: No real oil data available. Creating synthetic data for demonstration.\")\n",
    "    print(\"This should only be used for testing. For production, obtain the DCOILWTICO.csv and DCOILBRENTEU.csv files.\")\n",
    "\n",
    "    # Create date range matching freight data\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "\n",
    "    # Create synthetic oil prices with realistic values and volatility\n",
    "    # Base price around $70-80 with random walk\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    base_price = 75.0\n",
    "    random_walk = np.random.randn(len(date_range)).cumsum() * 2  # Random walk with std=2\n",
    "    synthetic_prices = base_price + random_walk\n",
    "\n",
    "    # Clip to reasonable range (50-120)\n",
    "    synthetic_prices = np.clip(synthetic_prices, 50, 120)\n",
    "\n",
    "    df_oil_wti = pd.DataFrame({\n",
    "        'WTI_Price': synthetic_prices\n",
    "    }, index=date_range)\n",
    "    \n",
    "    df_oil_brent = pd.DataFrame({\n",
    "        'Brent_Price': synthetic_prices + np.random.randn(len(date_range)) * 2  # Slight variation\n",
    "    }, index=date_range)\n",
    "\n",
    "    print(f\"Success: Created {len(df_oil_wti)} days of synthetic WTI oil price data\")\n",
    "    print(f\"Success: Created {len(df_oil_brent)} days of synthetic Brent EU oil price data\")\n",
    "    print(f\"  Date range: {df_oil_wti.index.min().strftime('%Y-%m-%d')} to {df_oil_wti.index.max().strftime('%Y-%m-%d')}\")\n",
    "    print(f\"  WTI Price range: ${df_oil_wti['WTI_Price'].min():.2f} to ${df_oil_wti['WTI_Price'].max():.2f}\")\n",
    "    print(f\"  Brent Price range: ${df_oil_brent['Brent_Price'].min():.2f} to ${df_oil_brent['Brent_Price'].max():.2f}\")\n",
    "    print(f\"  WTI Average: ${df_oil_wti['WTI_Price'].mean():.2f}\")\n",
    "    print(f\"  Brent Average: ${df_oil_brent['Brent_Price'].mean():.2f}\")\n",
    "    print(\"\\nWarning: This is SYNTHETIC data. Replace with real DCOILWTICO.csv and DCOILBRENTEU.csv data for actual predictions.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"OIL DATA LOADING COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total WTI days: {len(df_oil_wti)}\")\n",
    "print(f\"Total Brent EU days: {len(df_oil_brent)}\")\n",
    "print(f\"Data sources: {'EIA DCOILWTICO & DCOILBRENTEU' if not df_oil_wti.empty and not df_oil_brent.empty else 'Synthetic'}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f759cc",
   "metadata": {},
   "source": [
    "## Step 6: Load the new \"black swan\" geopolitical disruption data\n",
    "\n",
    "We now load the new, richer BigQuery dataset that contains specific geopolitical and black swan event metrics for shipping-critical regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b00bcc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOADING GEOPOLITICAL DISRUPTION DATA\n",
      "======================================================================\n",
      "\n",
      "--- Loading GDELT BigQuery data ---\n",
      "Success: Loaded 385 weeks of geopolitical disruption data from data/raw/bq-results-20251021-090045-1761037274833.csv\n",
      "  Date range: 2018-01-05 to 2025-08-22\n",
      "  Columns: 21 disruption metrics\n",
      "  Key metrics: ['global_total_events', 'global_disruption_events', 'extreme_crisis_events', 'high_velocity_media_events', 'black_swan_candidate_events', 'global_avg_impact', 'global_worst_event_impact', 'global_avg_sentiment']\n",
      "\n",
      "First 3 rows of GDELT data:\n",
      "            global_total_events  global_disruption_events  \\\n",
      "week_start                                                  \n",
      "2018-01-05               251442                     70842   \n",
      "2018-01-12               296571                     81076   \n",
      "2018-01-19               312641                     84236   \n",
      "\n",
      "            extreme_crisis_events  high_velocity_media_events  \\\n",
      "week_start                                                      \n",
      "2018-01-05                  19044                          53   \n",
      "2018-01-12                  22254                          27   \n",
      "2018-01-19                  25340                          48   \n",
      "\n",
      "            black_swan_candidate_events  global_avg_impact  \\\n",
      "week_start                                                   \n",
      "2018-01-05                            8           0.533128   \n",
      "2018-01-12                            5           0.576733   \n",
      "2018-01-19                            5           0.582549   \n",
      "\n",
      "            global_worst_event_impact  global_avg_sentiment  \\\n",
      "week_start                                                    \n",
      "2018-01-05                        -10             -2.197484   \n",
      "2018-01-12                        -10             -2.007298   \n",
      "2018-01-19                        -10             -1.900899   \n",
      "\n",
      "            global_total_media_mentions  global_peak_event_media  ...  \\\n",
      "week_start                                                        ...   \n",
      "2018-01-05                      1510540                      736  ...   \n",
      "2018-01-12                      1758585                      340  ...   \n",
      "2018-01-19                      1870024                     1070  ...   \n",
      "\n",
      "            infrastructure_attack_events  trade_restriction_events  \\\n",
      "week_start                                                           \n",
      "2018-01-05                           403                     17003   \n",
      "2018-01-12                           339                     18105   \n",
      "2018-01-19                           388                     18214   \n",
      "\n",
      "            protest_events  middle_east_disruption  asia_disruption  \\\n",
      "week_start                                                            \n",
      "2018-01-05            2689                    6053             1243   \n",
      "2018-01-12            3245                    4922             1479   \n",
      "2018-01-19            2915                    4060             1637   \n",
      "\n",
      "            europe_disruption  russia_ukraine_disruption  egypt_disruption  \\\n",
      "week_start                                                                   \n",
      "2018-01-05               1116                       3109               736   \n",
      "2018-01-12               1282                       3708               603   \n",
      "2018-01-19               1478                       3860               644   \n",
      "\n",
      "            yemen_disruption  unique_sources  \n",
      "week_start                                    \n",
      "2018-01-05               549           74902  \n",
      "2018-01-12               385           87655  \n",
      "2018-01-19               412           92107  \n",
      "\n",
      "[3 rows x 21 columns]\n",
      "\n",
      "======================================================================\n",
      "GEOPOLITICAL DATA LOADING COMPLETE\n",
      "======================================================================\n",
      "Total weeks: 385\n",
      "Data source: GDELT BigQuery\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"LOADING GEOPOLITICAL DISRUPTION DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load geopolitical disruption data from GDELT BigQuery export\n",
    "print(\"\\n--- Loading GDELT BigQuery data ---\")\n",
    "\n",
    "try:\n",
    "    # Try the actual filename first (bq-results from BigQuery)\n",
    "    gdelt_file_path = 'data/raw/bq-results-20251021-090045-1761037274833.csv'\n",
    "    df_gdelt_raw = pd.read_csv(gdelt_file_path, parse_dates=['event_date'])\n",
    "    \n",
    "    # Rename event_date to match expected column name\n",
    "    df_gdelt_raw = df_gdelt_raw.rename(columns={'event_date': 'week_start'})\n",
    "    \n",
    "    # Set the week_start as the index\n",
    "    df_gdelt = df_gdelt_raw.set_index('week_start')\n",
    "    \n",
    "    # Filter to match freight data date range\n",
    "    df_gdelt = df_gdelt[(df_gdelt.index >= start_date) & (df_gdelt.index <= end_date)].copy()\n",
    "    \n",
    "    if not df_gdelt.empty:\n",
    "        print(f\"Success: Loaded {len(df_gdelt)} weeks of geopolitical disruption data from {gdelt_file_path}\")\n",
    "        print(f\"  Date range: {df_gdelt.index.min().strftime('%Y-%m-%d')} to {df_gdelt.index.max().strftime('%Y-%m-%d')}\")\n",
    "        print(f\"  Columns: {len(df_gdelt.columns)} disruption metrics\")\n",
    "        print(f\"  Key metrics: {list(df_gdelt.columns[:8])}\")\n",
    "        print(\"\\nFirst 3 rows of GDELT data:\")\n",
    "        print(df_gdelt.head(3))\n",
    "    else:\n",
    "        print(f\"Error: No valid GDELT data found in date range\")\n",
    "        df_gdelt = pd.DataFrame()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{gdelt_file_path}' was not found.\")\n",
    "    print(\"Note: This should be the BigQuery export CSV file in the data/raw folder.\")\n",
    "    print(\"Expected columns: event_date, global_total_events, disruption metrics, regional disruption columns\")\n",
    "    \n",
    "    # Create an empty dataframe as fallback\n",
    "    df_gdelt = pd.DataFrame()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: An error occurred while loading GDELT data: {str(e)[:150]}\")\n",
    "    df_gdelt = pd.DataFrame()\n",
    "\n",
    "# Fallback: Create synthetic geopolitical disruption data if loading failed\n",
    "if df_gdelt.empty:\n",
    "    print(\"\\n--- Fallback: Creating synthetic placeholder data ---\")\n",
    "    print(\"Warning: No real GDELT data available. Creating synthetic data for demonstration.\")\n",
    "    print(\"This should only be used for testing. For production, obtain the GDELT BigQuery export CSV.\")\n",
    "    \n",
    "    # Create weekly date range matching freight data\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='W-FRI')  # Weekly on Fridays\n",
    "    \n",
    "    # Create synthetic disruption metrics\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    \n",
    "    df_gdelt = pd.DataFrame({\n",
    "        'conflict_events': np.random.poisson(5, len(date_range)),\n",
    "        'severe_incidents': np.random.poisson(2, len(date_range)),\n",
    "        'avg_tone': np.random.randn(len(date_range)) * 2 - 1,\n",
    "        'event_density': np.random.uniform(0.1, 0.9, len(date_range)),\n",
    "        'volatility_index': np.random.uniform(0, 100, len(date_range)),\n",
    "        'yemen_disruption': np.random.poisson(3, len(date_range)),\n",
    "        'egypt_disruption': np.random.poisson(2, len(date_range)),\n",
    "        'asia_disruption': np.random.poisson(4, len(date_range)),\n",
    "        'europe_disruption': np.random.poisson(3, len(date_range)),\n",
    "        'maritime_conflict_events': np.random.poisson(4, len(date_range)),\n",
    "        'extreme_crisis_events': np.random.poisson(2, len(date_range)),\n",
    "        'black_swan_candidate_events': np.random.poisson(1, len(date_range))\n",
    "    }, index=date_range)\n",
    "    \n",
    "    print(f\"Success: Created {len(df_gdelt)} weeks of synthetic geopolitical disruption data\")\n",
    "    print(f\"  Date range: {df_gdelt.index.min().strftime('%Y-%m-%d')} to {df_gdelt.index.max().strftime('%Y-%m-%d')}\")\n",
    "    print(f\"  Columns: {df_gdelt.columns.tolist()}\")\n",
    "    print(\"\\nWarning: This is SYNTHETIC data. Replace with real GDELT BigQuery export for actual predictions.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"GEOPOLITICAL DATA LOADING COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total weeks: {len(df_gdelt)}\")\n",
    "print(f\"Data source: {'GDELT BigQuery' if len(df_gdelt) > 0 else 'None'}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015d01da",
   "metadata": {},
   "source": [
    "## Step 7: Process New Datasets (Shanghai Port Activity, Chokepoints, China Monthly Trade)\n",
    "\n",
    "Now we'll load and aggregate the three new datasets to weekly frequency to match our freight data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6424ce02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PROCESSING NEW DATASETS\n",
      "======================================================================\n",
      "\n",
      "--- Processing china_monthly_trade.csv ---\n",
      "Loaded 77 rows\n",
      "  Date range: 2019-01-01 to 2025-05-01\n",
      "  Expanded to 330 weekly rows\n",
      "✓ China monthly trade processed successfully\n",
      "\n",
      "--- Processing china_port_activity.csv ---\n",
      "Loaded 2496 rows\n",
      "  Shanghai rows: 2496\n",
      "  Aggregated to 357 weekly rows with 22 features\n",
      "✓ Shanghai port activity processed successfully\n",
      "\n",
      "--- Processing chokepoints.csv ---\n",
      "Loaded 14988 rows\n",
      "  Unique chokepoints: ['Suez Canal', 'Bab el-Mandeb Strait', 'Malacca Strait', 'Gibraltar Strait', 'Dover Strait', 'Taiwan Strait']\n",
      "  Relevant chokepoint rows: 14988\n",
      "  Aggregated to 358 weekly rows with 24 features\n",
      "✓ Chokepoints processed successfully\n",
      "\n",
      "======================================================================\n",
      "NEW DATA PROCESSING COMPLETE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"PROCESSING NEW DATASETS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1) China Monthly Trade - validate, sort, expand to weekly\n",
    "print(\"\\n--- Processing china_monthly_trade.csv ---\")\n",
    "try:\n",
    "    df_trade = pd.read_csv('data/raw/china_monthly_trade.csv')\n",
    "    df_trade['date'] = pd.to_datetime(df_trade['date'], errors='coerce')\n",
    "    print(f\"Loaded {len(df_trade)} rows\")\n",
    "    \n",
    "    # Check for invalid dates\n",
    "    missing_dates = df_trade['date'].isna().sum()\n",
    "    if missing_dates > 0:\n",
    "        print(f\"  Warning: {missing_dates} rows with invalid dates (will be dropped)\")\n",
    "        df_trade = df_trade.dropna(subset=['date']).copy()\n",
    "    \n",
    "    # Check duplicates\n",
    "    dup_count = df_trade['date'].duplicated().sum()\n",
    "    if dup_count > 0:\n",
    "        print(f\"  Warning: {dup_count} duplicate months found (keeping first)\")\n",
    "        df_trade = df_trade.drop_duplicates(subset=['date'], keep='first')\n",
    "    \n",
    "    # Sort chronologically\n",
    "    df_trade = df_trade.sort_values('date').reset_index(drop=True)\n",
    "    print(f\"  Date range: {df_trade['date'].min().date()} to {df_trade['date'].max().date()}\")\n",
    "    \n",
    "    # Expand monthly to weekly (forward-fill)\n",
    "    df_trade_indexed = df_trade.set_index('date').sort_index()\n",
    "    weekly_idx = pd.date_range(start=df_trade_indexed.index.min(), \n",
    "                                end=df_trade_indexed.index.max(), freq='W-FRI')\n",
    "    df_trade_weekly = df_trade_indexed.reindex(weekly_idx).ffill()\n",
    "    df_trade_weekly.index.name = 'date'\n",
    "    \n",
    "    print(f\"  Expanded to {len(df_trade_weekly)} weekly rows\")\n",
    "    print(f\"✓ China monthly trade processed successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error processing china_monthly_trade.csv: {str(e)}\")\n",
    "    df_trade_weekly = pd.DataFrame()\n",
    "\n",
    "# 2) Shanghai Port Activity - filter Shanghai, aggregate to weekly\n",
    "print(\"\\n--- Processing china_port_activity.csv ---\")\n",
    "try:\n",
    "    df_port = pd.read_csv('data/raw/china_port_activity.csv', parse_dates=['date'])\n",
    "    print(f\"Loaded {len(df_port)} rows\")\n",
    "    \n",
    "    # Filter Shanghai rows (portid=port2027 or portname contains Shanghai)\n",
    "    mask_sh = (df_port['portname'].str.contains('Shanghai', case=False, na=False) | \n",
    "               (df_port['portid'] == 'port2027'))\n",
    "    df_sh = df_port[mask_sh].copy()\n",
    "    print(f\"  Shanghai rows: {len(df_sh)}\")\n",
    "    \n",
    "    if len(df_sh) == 0:\n",
    "        print(\"  Warning: No Shanghai rows found, skipping\")\n",
    "        df_sh_weekly = pd.DataFrame()\n",
    "    else:\n",
    "        # Aggregate to weekly\n",
    "        df_sh.set_index('date', inplace=True)\n",
    "        \n",
    "        # Weekly mean for port calls, weekly sum for volumes\n",
    "        portcall_cols = [c for c in df_sh.columns if c.startswith('portcalls')]\n",
    "        volume_cols = [c for c in df_sh.columns if c.startswith(('import','export'))]\n",
    "        \n",
    "        weekly_mean = df_sh[portcall_cols].resample('W-FRI').mean()\n",
    "        weekly_sum = df_sh[volume_cols].resample('W-FRI').sum()\n",
    "        \n",
    "        df_sh_weekly = weekly_mean.join(weekly_sum)\n",
    "        \n",
    "        # Create congestion indicator (normalized container portcalls)\n",
    "        if 'portcalls_container' in weekly_mean.columns:\n",
    "            pcm = weekly_mean['portcalls_container']\n",
    "            df_sh_weekly['portcalls_container_norm'] = (pcm - pcm.mean()) / (pcm.std() if pcm.std() > 0 else 1)\n",
    "        \n",
    "        print(f\"  Aggregated to {len(df_sh_weekly)} weekly rows with {len(df_sh_weekly.columns)} features\")\n",
    "        print(f\"✓ Shanghai port activity processed successfully\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error processing china_port_activity.csv: {str(e)}\")\n",
    "    df_sh_weekly = pd.DataFrame()\n",
    "\n",
    "# 3) Chokepoints - filter relevant chokepoints, aggregate to weekly\n",
    "print(\"\\n--- Processing chokepoints.csv ---\")\n",
    "try:\n",
    "    df_choke = pd.read_csv('data/raw/chokepoints.csv', parse_dates=['date'])\n",
    "    print(f\"Loaded {len(df_choke)} rows\")\n",
    "    \n",
    "    # Show unique chokepoints\n",
    "    unique_choke = df_choke['portname'].dropna().unique()\n",
    "    print(f\"  Unique chokepoints: {list(unique_choke)}\")\n",
    "    \n",
    "    # Filter for Shanghai->EU route chokepoints\n",
    "    keywords = ['suez', 'malacca', 'strait', 'red sea', 'bab', 'gibraltar', 'dover']\n",
    "    mask = df_choke['portname'].str.lower().str.contains('|'.join(keywords), na=False)\n",
    "    df_relevant = df_choke[mask].copy()\n",
    "    print(f\"  Relevant chokepoint rows: {len(df_relevant)}\")\n",
    "    \n",
    "    if len(df_relevant) == 0:\n",
    "        print(\"  Warning: No relevant chokepoints found\")\n",
    "        df_choke_weekly = pd.DataFrame()\n",
    "    else:\n",
    "        # Aggregate weekly per chokepoint, then pivot\n",
    "        df_relevant.set_index('date', inplace=True)\n",
    "        \n",
    "        # Group by portname, resample weekly, compute mean for key metrics\n",
    "        weekly_stats = df_relevant.groupby('portname').resample('W-FRI').agg({\n",
    "            'n_container': 'mean',\n",
    "            'n_total': 'mean',\n",
    "            'capacity_container': 'mean',\n",
    "            'capacity': 'mean'\n",
    "        })\n",
    "        \n",
    "        # Pivot so each chokepoint becomes columns\n",
    "        df_choke_weekly = weekly_stats.unstack(level=0)\n",
    "        \n",
    "        # Flatten column names (e.g., 'suez_canal_n_container')\n",
    "        df_choke_weekly.columns = [f\"{col[1].replace(' ','_').lower()}_{col[0]}\" \n",
    "                                    for col in df_choke_weekly.columns]\n",
    "        \n",
    "        print(f\"  Aggregated to {len(df_choke_weekly)} weekly rows with {len(df_choke_weekly.columns)} features\")\n",
    "        print(f\"✓ Chokepoints processed successfully\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error processing chokepoints.csv: {str(e)}\")\n",
    "    df_choke_weekly = pd.DataFrame()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"NEW DATA PROCESSING COMPLETE\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final_step",
   "metadata": {},
   "source": [
    "## Step 8: Save all collected data\n",
    "\n",
    "Now that we have loaded and cleaned all our data sources, let's save them to CSV files for use in the next phase of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "save_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SAVING ALL COLLECTED DATA TO data/processed/\n",
      "======================================================================\n",
      "\n",
      "✓ Saved freight data to data/processed/freight_data.csv\n",
      "  Rows: 385, Columns: 2\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_oil_wti' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Rows: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df_freight)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Columns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df_freight.columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Save WTI oil data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mdf_oil_wti\u001b[49m.empty:\n\u001b[32m     17\u001b[39m     wti_output = \u001b[33m'\u001b[39m\u001b[33mdata/processed/oil_wti.csv\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     18\u001b[39m     df_oil_wti.to_csv(wti_output)\n",
      "\u001b[31mNameError\u001b[39m: name 'df_oil_wti' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"SAVING ALL COLLECTED DATA TO data/processed/\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create output directory\n",
    "import os\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "\n",
    "# Save freight data\n",
    "freight_output = 'data/processed/freight_data.csv'\n",
    "df_freight.to_csv(freight_output)\n",
    "print(f\"\\n✓ Saved freight data to {freight_output}\")\n",
    "print(f\"  Rows: {len(df_freight)}, Columns: {len(df_freight.columns)}\")\n",
    "\n",
    "# Save WTI oil data\n",
    "if not df_oil_wti.empty:\n",
    "    wti_output = 'data/processed/oil_wti.csv'\n",
    "    df_oil_wti.to_csv(wti_output)\n",
    "    print(f\"\\n✓ Saved WTI oil data to {wti_output}\")\n",
    "    print(f\"  Rows: {len(df_oil_wti)}, Columns: {len(df_oil_wti.columns)}\")\n",
    "\n",
    "# Save Brent oil data\n",
    "if not df_oil_brent.empty:\n",
    "    brent_output = 'data/processed/oil_brent.csv'\n",
    "    df_oil_brent.to_csv(brent_output)\n",
    "    print(f\"\\n✓ Saved Brent oil data to {brent_output}\")\n",
    "    print(f\"  Rows: {len(df_oil_brent)}, Columns: {len(df_oil_brent.columns)}\")\n",
    "\n",
    "# Save geopolitical data\n",
    "if not df_gdelt.empty:\n",
    "    gdelt_output = 'data/processed/geopolitical_data.csv'\n",
    "    df_gdelt.to_csv(gdelt_output)\n",
    "    print(f\"\\n✓ Saved geopolitical data to {gdelt_output}\")\n",
    "    print(f\"  Rows: {len(df_gdelt)}, Columns: {len(df_gdelt.columns)}\")\n",
    "\n",
    "# Save new processed datasets\n",
    "if not df_trade_weekly.empty:\n",
    "    trade_output = 'data/processed/china_monthly_trade_weekly.csv'\n",
    "    df_trade_weekly.to_csv(trade_output)\n",
    "    print(f\"\\n✓ Saved China monthly trade (weekly) to {trade_output}\")\n",
    "    print(f\"  Rows: {len(df_trade_weekly)}, Columns: {len(df_trade_weekly.columns)}\")\n",
    "\n",
    "if not df_sh_weekly.empty:\n",
    "    shanghai_output = 'data/processed/shanghai_port_weekly.csv'\n",
    "    df_sh_weekly.to_csv(shanghai_output)\n",
    "    print(f\"\\n✓ Saved Shanghai port activity (weekly) to {shanghai_output}\")\n",
    "    print(f\"  Rows: {len(df_sh_weekly)}, Columns: {len(df_sh_weekly.columns)}\")\n",
    "\n",
    "if not df_choke_weekly.empty:\n",
    "    choke_output = 'data/processed/chokepoints_weekly.csv'\n",
    "    df_choke_weekly.to_csv(choke_output)\n",
    "    print(f\"\\n✓ Saved chokepoints (weekly) to {choke_output}\")\n",
    "    print(f\"  Rows: {len(df_choke_weekly)}, Columns: {len(df_choke_weekly.columns)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DATA COLLECTION COMPLETE!\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nAll data has been saved to data/processed/\")\n",
    "print(\"Next step: Run 02_data_understanding.ipynb for feature engineering and analysis.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
